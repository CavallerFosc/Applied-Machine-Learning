{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Different Packages Required\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import nltk for using natural language processing techniques\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In this section, we read the data from \n",
    "# https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip(downloaded and \n",
    "# unzipped) and label it accordingly.\n",
    "\n",
    "\n",
    "messages=pd.read_csv('SMSSpamCollection', sep='\\t', names= [\"label\",\"sms\"])\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refining the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this section, we drop the duplicate messages\n",
    "\n",
    "messages.drop_duplicates(subset='sms', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are redundant, just to check the messages\n",
    "messages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.groupby('label').count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function at first tokenizes the sentences into words, then removes all the punctuations and the stopwords.\n",
    "# Then it does stemming e.g. \"go\", \"going\", \"goes\" all these words are replaced with its root word \"go\".\n",
    "\n",
    "def refinement(text):\n",
    "    token = TweetTokenizer()\n",
    "    wordsandpuncts = token.tokenize(text)\n",
    "    puncts = [\"'\", \"!\",\"?\",\",\",\".\",\"@\", \"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\"(\",\")\",\"<\",\">\",\"/\",\";\",\":\",\"|\"]\n",
    "    words = [x for x in wordsandpuncts if x not in puncts]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    start_words = [w for w in words if not w in stop_words]\n",
    "    stemmer = PorterStemmer() \n",
    "    clean_words=[stemmer.stem(word) for word in start_words]                 \n",
    "    \n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just for checking how does the function work?\n",
    "messages['sms'].apply(refinement).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = messages['sms']\n",
    "y = messages['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "#print(\"Shape of X is {}\".format(X.shape))\n",
    "#print(\"Shape of X_train is {} and shape of y_train is {}\".format(X_train.shape, y_train.shape))\n",
    "#print(\"Shape of X_test is {} and shape of y_test is {}\".format(X_test.shape, y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Models with 3 Classifiers: NaiveBayes, SVM, Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the pipeline for our three classifiers:\n",
    "# 1. Naive-Bayes : This classifier check if certain word or a certain set of words appearing in a message\n",
    "#    to declare it as spam or ham. Some words have probabilities of appearing in spam and legitimate mail.\n",
    "#    It learns about these probabilities(it also considers the frequency of the words appearing).\n",
    "#    from the training batch. Then, given a message from the test set or the user, it checks the set of words \n",
    "#    appearing in the message and using Bayes theorem count the probability of the message being spam or ham.\n",
    "#\n",
    "# 2. SVM : SVM or Support Vector Machine checks the training data and tries to find a separating hyperplane\n",
    "#          between spams(negative points) and hams(positive points) which maximizes the distance of the plane\n",
    "#          from both the set of points.\n",
    "# \n",
    "# 3. Logistic Regression: Logistic Regression model indicates spam messages as 0 and ham messages as 1. It \n",
    "#                         measures the relationship between the categorical dependent variable and one or more \n",
    "#                         independent variables by estimating probabilities using a logistic function. So, from\n",
    "#                         the training set it tries to produce the logistic or the sigmoid function, that helps\n",
    "#                         determine if some messages from the test set is spam or ham, according to its logistic\n",
    "#                         function value is 0 or 1.\n",
    "#\n",
    "# We do these classifications using pipeline in three steps:\n",
    "# 1. First take the training set and create a bag of words calling our \"refinement\" function as an analyzer and\n",
    "#    convert the collection of text documents to a matrix of token counts using \"CountVectorizer\".\n",
    "# 2. Then, we produce the tf-idf vector from them.\n",
    "# 3. Then we use the desired classifier.\n",
    "\n",
    "NB = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=refinement)),\n",
    "    ('tfidf',TfidfTransformer()), \n",
    "    ('classifier',MultinomialNB())\n",
    "])\n",
    "\n",
    "SVM = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=refinement)),\n",
    "    ('tfidf',TfidfTransformer()), \n",
    "    ('classifier',LinearSVC())\n",
    "])\n",
    "\n",
    "LR = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=refinement)),\n",
    "    ('tfidf',TfidfTransformer()), \n",
    "    ('classifier',LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This produces 5-fold cross validation\n",
    "def cross_val(i):\n",
    "    print(\"The cross validation scores are: {}\".format(cross_val_score(i, X=X_train, y=y_train, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit training data to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit and train the data in the pipeline\n",
    "def fitting(i):\n",
    "    i.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Evauations on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we use the model on the test data\n",
    "def final_result(i):\n",
    "    y_test_predicted = i.predict(X_test)\n",
    "\n",
    "#This produces the accuracy score\n",
    "    print(\"The fraction of correctly classified samples is {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
    "    print(\"The number of correctly classified samples is {}\".format(accuracy_score(y_test, y_test_predicted, normalize=False)))\n",
    "    print pd.DataFrame(confusion_matrix(y_test, y_test_predicted), \n",
    "                 index={'true ham', 'true spam'}, \n",
    "                 columns={'pred ham', 'pred spam'})\n",
    "\n",
    "#This produces the precision and recall on the test data\n",
    "    print(\"The classification report is the following:\")\n",
    "    print(classification_report(y_test,y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we ask for the user input for which classifier they want to use and print the cross-validation score and\n",
    "# also the final evaluations on the test data. This also produces the classification report, which contains\n",
    "# precision and recall\n",
    "def classifier(i):\n",
    "    if i==NB:\n",
    "        print(\"The classifier used here is Naive-Bayes.\")\n",
    "    elif i== SVM:\n",
    "        print(\"The classifier used here is SVM.\")\n",
    "    elif i== LR:\n",
    "        print(\"The classifier used here is Logistic Regression.\")\n",
    "    cross_val(i)\n",
    "    fitting (i)\n",
    "    final_result(i)\n",
    "note= '''This is the code for spam filter. We have used three classifier. Choose which classifier you\n",
    "         want to use:\n",
    "         Enter 1 for Naive-Bayes\n",
    "         Enter 2 for SVM\n",
    "         Enter 3 for Logistic Regression'''\n",
    "\n",
    "print(note)\n",
    "switch = int(input(\"Enter choice: \"))\n",
    "\n",
    "if switch == 1:\n",
    "    classifier(NB)\n",
    "elif switch == 2:\n",
    "    classifier(SVM)\n",
    "elif switch == 3:\n",
    "    classifier(LR)\n",
    "else:\n",
    "    print(\"wrong choice\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
